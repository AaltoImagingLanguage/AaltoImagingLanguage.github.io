<!DOCTYPE html>
	<html lang="en">
	

	<head>
	

	    <meta charset="utf-8">
	    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	    <meta name="description" content="">
	    <meta name="author" content="">
	

	    <title>Speechness</title>
	

	    <!-- Bootstrap core CSS -->
	    <link href="../vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">
	

	    <!-- Custom styles for this template -->
	    <link href="../css/simple-sidebar.css" rel="stylesheet">
	

	</head>
	

	<body>
	

	    <div id="wrapper">
	

	        <!-- Sidebar -->
	        <div id="sidebar-wrapper">
	            <ul class="sidebar-nav">
	                <li>
	                    <a href="#">Home</a>
	                </li>
	                <li>
	                    <a href="https://github.com/AaltoImagingLanguage/speechness">Github</a>
	                </li>
                <li>
                    <a href="reports.html">Supplementary results</a>
                </li>
	                <li>
	                    <a href="contact.html">More information </a>
	                </li>
	                <li>
	                    <a href="../index.html">Back to project menu</a>
	                </li>
	

	            </ul>
	        </div>
	        <!-- /#sidebar-wrapper -->
	        <script type='text/javascript' src='https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js'></script>
	        <!-- Page Content -->
	        <div id="page-content-wrapper">
	            <div class="container-fluid">



<h1>Areas contributing to acoustic decoding of spoken words and environmental sounds show more overlap than areas contributing to semantic decoding of the two classes of sounds</h1>
<p>Successful decoding of the time-evolving spectrogram and phoneme content of spoken words at 100–180 ms lag in the convolution model engaged cortical areas around the bilateral primary auditory cortices, with contributions from superior temporal and inferior frontal areas. Similar areas were involved in decoding the MPS of environmental sounds (at 50–100 ms); 9 out of 20 most predictive areas for acoustic decoding were shared between the two sound groups. 
Semantic decoding of environmental sounds (at 650–700 ms) had strongest contributions from bilateral superior temporal areas, areas surrounding the bilateral central sulci, parietal cortex, as well as right frontal and insular areas. For spoken words, sources that contributed most to the decoding concentrated on areas surrounding occipito-temporal cortex, bilateral frontal cortex, left central sulcus and insular areas. Two out of 20 most predictive areas for semantic decoding were shared between the two sound groups.</p>
<b><img src="Source_level_decoding_results.jpg" width="800">
</b>
<b>Illustration of the top 20 ranking cortical areas (parcels) for the decoding of acoustic and semantic features for spoken words and for environmental sounds. The parcel names are listed in the table below. Sources for acoustic decoding of spoken words are based on the convolution model and spectrogram, with the best lag (100–180 ms). Sources for acoustic decoding of environmental sounds are based on the regression model and MPS, at 50–100 ms. For semantic decoding, the regression model at 650–700 ms was used for both classes of sounds.
</b>			    
        </div>
<h1>Acoustic similarities of environmental sound sources can enhance their semantic decoding</h1>
<p>Environmental sounds have more acoustic variation than spoken words, and typically more variation across categories than within categories, as sounds with a similar source (e.g. animal vocalizations vs. tool sounds) are more likely to share acoustic features such as frequency content and degree of harmonics or periodicity. This could be reflected in the MEG responses and, in theory, improve the semantic decoding of environmental sounds on category level. To investigate this possibility, we compared the dissimilarity matrices of the semantic features and acoustic features for both stimulus groups with a Mantel test. Some overlap was found in the category structures of acoustic and semantic features for environmental sounds (p = 0.004) but not for spoken words (p = 0.80). This is possibly reflected in the category-level decoding of semantic features that was significantly better than item-level decoding particularly for environmental sounds (Z = 3.4, p = 0.00015). However, the better performance for category-level than item-level decoding was not entirely due to acoustic within-category similarity and between-category differences (in environmental sounds), since this difference was also observed to some extent for spoken words (Z = 2.2, p = 0.025); thus it seems to be partly due to salient cortical representations of the semantic categories.</p>
</b>			    
<b><img src="Feature_similarities.jpg" width="800">
</b> 
<b>a) Dissimilarity (1-correlation) of semantic features, the same for both types of sounds, cluster into salient categories (except for the category ‘other’). b) Dissimilarity (1-correlation) of acoustic (MPS) features of environmental sounds. c) Dissimilarity (1-correlation) of acoustic (MPS) features of spoken words. d) Predicting the category (left) or identity (right) of environmental sounds and spoken words based on semantic features. Each thin line represents the decoding performance in one participant and the thick line the grand average over all 16 participants. The grey solid line denotes chance level performance (50%).
</b>			    
        </div>
<h1>Acoustic speaker characteristics influence modulation power spectrum but not spectrogram decoding performance</h1>
<p>We tested whether acoustic characteristics of the 8 different speakers contributed to decoding performance for the spoken words. In within-speaker decoding the two test items are always chosen from the same speaker; in across-speaker-decoding the two test items are always chosen from different speakers. The frequency and modulation content characteristic of each speaker contributed significantly to the MPS-based decoding of spoken words: items spoken by the same speaker could not be distinguished at a very high accuracy (within-speaker decoding of spoken words 58%), whereas the model performed significantly better when only item pairs from different speakers were considered (across-speaker decoding 66%; within-speaker vs. across-speaker decoding: Z = 2.4, p = 0.016). In contrast, the spectrogram convolution model could equally well tell apart items spoken by the same or different speakers (within-speaker decoding 84%, across-speaker decoding 83%; Z = 0.62, p = 0.56). Thus, acoustic features other than the acoustic speaker characteristics play an important role in the decoding of spoken words with the convolution model. </p>
<b><img src="Within_vs_across_speaker_decoding.jpg" width="800">
<b>Decoding results for spoken word acoustics with the MPS regression model (left), spectrogram convolution model (center), and the phoneme content of spoken words with the convolution model (right). Each thin line represents the decoding performance in one participant and the thick line the grand average over 16 participants. The grey solid line denotes chance level performance (50%).
</b>		   
	        </div>
	        <!-- /#page-content-wrapper -->
	

	    </div>
	    <!-- /#wrapper -->
	

	    <!-- Bootstrap core JavaScript -->
	    <script src="../vendor/jquery/jquery.min.js"></script>
	    <script src="../vendor/bootstrap/js/bootstrap.bundle.min.js"></script>
	

	    <!-- Menu Toggle Script -->
	    <script>
	    $("#menu-toggle").click(function(e) {
	        e.preventDefault();
	        $("#wrapper").toggleClass("toggled");
	    }); </script>
	

	</body>


	</html>
